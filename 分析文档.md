递归神经网络(RNN)是神经网络的一种，被广泛用于自然语言处理与手写字体的识别中。本次的分析工作主要分为以下几部分：
* 使用递归神经网络和其变体(长短期记忆网络LSTM、GRU)在多个数据集(MNIST/FashionMNIST/CIFAR10)上进行训练、测试，比较效果
* 简单分析不同数据集给参数调整(RNN网络层数、训练迭代次数)及过拟合现象带来的影响
* 分析其他超参数(学习率/隐藏层节点数/Dropout)对训练结果带来的影响

## 比较RNN/LSTM/GRU这三种递归神经网络的效果
普通的RNN神经网络结构存在着梯度消失或者梯度爆炸的问题，对权值更新和学习效果带来不利的影响。为了解决梯度消失与梯度爆炸的问题，LSTM及其简化版本GRU被相继提出，并获得了不错的效果。
**工作：**将RNN/LSTM/GRU应用于MNIST测试集上，比较其学习效果。
<div align=center><img width="450" height="450" src="https://github.com/DICKIEZhu/Machine_learning/blob/master/1-1.png"/></div>
<div align=center>图 1-1 RNN/LSTM/GRU在MNIST上的训练过程</div>
<div align=center><img width="450" height="450" src="https://github.com/DICKIEZhu/Machine_learning/blob/master/1-2.png"/></div>
<div align=center>图 1-2 递归网络层数\*4的RNN/LSTM/GRU在MNIST上的训练过程</div>
<div align=center><img width="450" height="450" src="https://github.com/DICKIEZhu/Machine_learning/blob/master/1-3.png"/></div>
<div align=center>图 1-3 迭代训练次数\*4的RNN/LSTM/GRU在MNIST上的训练过程</div><br>
**结论：**在三组实验下，LSTM与GRU均取得了比普通RNN更高的识别准确率，但在本次实验中，提升训练阶段的迭代次数后，RNN与LSTM、GRU之间的差距出现了明显减小。

## 简单分析数据集的复杂程度对递归神经网络的部分参数选择和过拟合现象的影响
**思考1：**在前面在MNIST的实验中，增加递归网络的层数反而使得RNN/LSTM/GRU的效果变差。本次采用的三种数据集MNIST->FashionMNIST->CIFAR10，图像内容由手写数字->黑白物品->彩色物品与动物，复杂度逐渐提升，这是否意味着需要更多的递归网络层数，以获取更复杂的特征？
<div align=center><img width="450" height="450" src="https://github.com/DICKIEZhu/Machine_learning/blob/master/2-1.png"/></div>
<div align=center>图 2-1 RNN/LSTM/GRU在FashionMNIST上的训练过程</div>
<div align=center><img width="450" height="450" src="https://github.com/DICKIEZhu/Machine_learning/blob/master/2-2.png"/></div>
<div align=center>图 2-2 递归网络层数\*4的RNN/LSTM/GRU在FashionMNIST上的训练过程</div>
<div align=center><img width="450" height="450" src="https://github.com/DICKIEZhu/Machine_learning/blob/master/2-3.png"/></div>
<div align=center>图 3-3 迭代训练次数\*4的RNN/LSTM/GRU在FashionMNIST上的训练过程</div><br>
**结论：**结论：在MNIST/FashionMNIST/CIFAR10三个数据集中，无论是普通的RNN，还是LSTM、GRU，其递归神经网络部分的层数的增加(2层->8层)均未能提升网络性能，相反，8层的递归神经网络甚至使普通RNN网络在FashionMNIST数据集上的性能大幅降低。<br>

**思考2：**如果数据集越简单，是否意味着需要更早地停止迭代训练，以避免过拟合现象的发生？
**结果：**该部分实验由LSTM在MNIST/FashionMNIST/CIFAR10三个数据集上迭代训练32次。实验的结果与我的主观设想几乎相反，在最简单的MNIST数据集中，即使是迭代训练了32次(学习效果几乎已经停止上升)，LSTM在测试集中的损失值与训练的损失值非常接近。这表明即使是在冗余的训练后，LSTM网络并没有出现比较严重的过拟合现象，仍然具备较好的泛化效果。相反，在CIFAR10数据集中，在迭代训练了32次后，LSTM网络在测试集中的损失值要明显大于其在训练集中的损失值。这表明经过多次训练后，LSTM网络已经出现了比较明显的过拟合现象。基于以上的实验现象，我认为当数据集更为复杂时(或者说包含更多特征时)，在训练阶段应该更为留意过拟合现象，控制合适的训练迭代次数，否则会出现比较严重的过拟合现象。

## 尝试其他超参数对递归神经网络的学习效果的影响
该部分实验由LSTM在CIFAR10数据集上迭代训练进行。
* 学习率：学习率作为最重要的超参数，对网络的训练效果往往起着决定性的作用。本实验分别采用0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001的6种学习率进行训练。实验结果显示0.03的学习率使得神经网络完全没有任何学习效果(准确率为10%)，0.001的学习率取得了最好的效果，随着学习率进一步降低，在训练中损失值的下降速度明显变缓，收敛速度降低，而且在测试集上的准确率也逐渐降低。
* 隐藏层节点数：分别采用了32, 64, 128, 256, 512的5种隐藏层节点数进行训练。实验结果显示，128个隐藏层节点数的递归神经网络取得最好的准确率(56%)，同时，更多的隐含层节点数也在训练中表现出更强的过拟合的倾向性。
* Dropout：Dropout指的是在训练的过程中，使某些神经元按照一定的概率不工作，旨在减小过拟合的发生。常用的Dropout值有0.3，0.5，0.7。本实验中采用0.1,0.3,0.5,0.7,0.9这5种dropout取值进行训练。实验结果显示，最大的dropout值可以显著降低过拟合现象，但也会降低网络的准确性，当dropout值为0.5时在CIFAR10数据集上取得较理想的效果。
